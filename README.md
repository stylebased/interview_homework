# interview_homework
Codebase Data Factory

A framework for automatically generating high-quality training datasets from real software projects â€” including:

Code Understanding (Q&A)

System Design / Architecture Reasoning

The pipeline:

Analyzes an existing codebase

Extracts structure and code snippets

Prompts an LLM to generate realistic tasks

Outputs structured datasets suitable for SFT (Supervised Fine-Tuning)

âœ¨ Features

âœ” Analyze any local or open-source repository

âœ” Two realistic scenarios

Scene-1: Code Q&A (understanding + reasoning)

Scene-2: System design + architecture planning

âœ” JSONL datasets for training

âœ” DRY-RUN testing mode (no LLM required)

âœ” Modular and extendable

ğŸ“‚ Project Structure
code_data_factory/
 â”œâ”€â”€ analyzer.py          # Analyze repository, extract structure + code chunks
 â”œâ”€â”€ scene1_pipeline.py   # Generate Code Q&A dataset
 â”œâ”€â”€ scene2_pipeline.py   # Generate System Design dataset
 â”œâ”€â”€ postprocess.py       # Clean + merge into final SFT dataset
 â”œâ”€â”€ llm_client.py        # Wrapper for local / HF / OpenAI models
 â”œâ”€â”€ config.py            # Configuration settings
 â””â”€â”€ cli.py               # Command-line interface


Generated data appears under:

data/
 â”œâ”€â”€ project_skeleton.txt
 â”œâ”€â”€ project_skeleton.json
 â”œâ”€â”€ chunks.json
 â”œâ”€â”€ scene1_raw.jsonl
 â”œâ”€â”€ scene1_sft.jsonl
 â”œâ”€â”€ scene2_raw.jsonl
 â”œâ”€â”€ scene2_sft.jsonl
 â””â”€â”€ combined_sft.jsonl

ğŸ›  Installation

Clone the repository:

git clone YOUR_REPO_URL
cd codebase-data-factory


Create a virtual environment:

python -m venv .venv
source .venv/bin/activate        # Windows: .venv\Scripts\activate
pip install -r requirements.txt

ğŸ¯ Select a Target Codebase

Clone any project you want to analyze, for example:

git clone https://github.com/halo-dev/halo.git halo-main


Set environment variable:

export TARGET_REPO_PATH=/path/to/halo-main


Windows PowerShell:

setx TARGET_REPO_PATH "C:\path\to\halo-main"

ğŸ§ª DRY-RUN Mode (Recommended First)

Run everything without calling an actual LLM:

export DRY_RUN=1


This tests pipeline logic safely.

ğŸš€ Usage
1ï¸âƒ£ Analyze repository
python main.py analyze

2ï¸âƒ£ Generate Scene-1 (Code Q&A)
python main.py scene1 --limit 20 --qa-count 3 --dry-run

3ï¸âƒ£ Generate Scene-2 (System Design)
python main.py scene2 --count 5 --dry-run

4ï¸âƒ£ Post-process and merge datasets
python main.py postprocess

ğŸ¤– Using a Real LLM (Optional)
Option A â€” HuggingFace Local Model
export DRY_RUN=0
export HF_MODEL_NAME="Qwen/Qwen2.5-7B-Instruct"

Option B â€” OpenAI-Compatible API
export DRY_RUN=0
export USE_OPENAI=1
export OPENAI_API_KEY=YOUR_KEY

ğŸ“Œ Output Overview
File	Description
project_skeleton.*	Compressed view of project structure
chunks.json	Extracted code segments
scene1_raw.jsonl	Raw Q&A responses
scene1_sft.jsonl	Cleaned dataset for Scene-1
scene2_raw.jsonl	Raw architecture designs
scene2_sft.jsonl	Cleaned dataset for Scene-2
combined_sft.jsonl	Final merged dataset
âœ… Why This Project Matters

This pipeline demonstrates:

automated dataset creation

support for multiple training tasks

realistic development scenarios

structured, reusable design

explainable outputs (thinking traces included)

ability to run without a model first (dry-run)

âš ï¸ Notes

Large repositories may take time to process

Avoid scanning folders like:

node_modules
build
dist
.git


Always test using DRY_RUN first
