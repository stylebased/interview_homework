# interview_homework
Codebase Data Factory

A framework for automatically generating high-quality training datasets from real software projects â€” including
code understanding (Q&A) and system design (architecture reasoning).

This repository builds a pipeline that:

Analyzes an existing codebase

Extracts structure and code snippets

Prompts an LLM to generate realistic tasks

Produces structured training datasets suitable for SFT (Supervised Fine-Tuning)

âœ¨ Features

âœ” Automatically analyze any local or open-source repository
âœ” Generate two training scenarios:

1ï¸âƒ£ Scene-1: Code Q&A (understanding, reasoning, business context)
2ï¸âƒ£ Scene-2: System design (architecture planning, explanation, trace)

âœ” Structured JSONL dataset outputs
âœ” DRY-RUN mode (no LLM required for testing)
âœ” Extensible and modular framework

ğŸ“‚ Project Structure
code_data_factory/
 â”œâ”€â”€ analyzer.py          # Analyze repository, build skeleton, extract chunks
 â”œâ”€â”€ scene1_pipeline.py   # Generate Code Q&A dataset
 â”œâ”€â”€ scene2_pipeline.py   # Generate System Design dataset
 â”œâ”€â”€ postprocess.py       # Clean and merge data into final SFT format
 â”œâ”€â”€ llm_client.py        # Wrapper for local/HF/OpenAI models
 â”œâ”€â”€ config.py            # Global configuration
 â””â”€â”€ cli.py               # Command line entrypoint


Generated data is stored under:

data/
 â”œâ”€â”€ project_skeleton.txt
 â”œâ”€â”€ project_skeleton.json
 â”œâ”€â”€ chunks.json
 â”œâ”€â”€ scene1_raw.jsonl
 â”œâ”€â”€ scene1_sft.jsonl
 â”œâ”€â”€ scene2_raw.jsonl
 â”œâ”€â”€ scene2_sft.jsonl
 â””â”€â”€ combined_sft.jsonl

ğŸ›  Installation

Clone the repository:

git clone YOUR_REPO_URL
cd codebase-data-factory


Create virtual environment:

python -m venv .venv
source .venv/bin/activate        # Windows: .venv\Scripts\activate
pip install -r requirements.txt

ğŸ¯ Select a Target Codebase

Clone any public project (or your own project):

Example:

git clone https://github.com/halo-dev/halo.git halo-main


Set environment variable:

export TARGET_REPO_PATH=/path/to/halo-main


Windows PowerShell:

setx TARGET_REPO_PATH "C:\path\to\halo-main"

ğŸ§ª DRY-RUN Mode (recommended first)

Run the full pipeline without calling any real LLM:

export DRY_RUN=1


This allows you to test pipeline logic safely.

ğŸš€ Usage
Step 1 â€” Analyze repository
python main.py analyze

Step 2 â€” Generate Scene-1 (Code Q&A)
python main.py scene1 --limit 20 --qa-count 3 --dry-run

Step 3 â€” Generate Scene-2 (System Design)
python main.py scene2 --count 5 --dry-run

Step 4 â€” Post-process and merge datasets
python main.py postprocess

ğŸ¤– Using a real LLM (optional)
Option A â€” HuggingFace local model

In environment:

export DRY_RUN=0
export HF_MODEL_NAME="Qwen/Qwen2.5-7B-Instruct"

Option B â€” OpenAI-compatible API
export DRY_RUN=0
export USE_OPENAI=1
export OPENAI_API_KEY=YOUR_KEY

ğŸ“Œ Output Overview

This project automatically produces:

File	Description
project_skeleton.*	Compact representation of project structure
chunks.json	Extracted code segments
scene1_raw.jsonl	Raw Q&A generation
scene1_sft.jsonl	Cleaned SFT dataset (Scene-1)
scene2_raw.jsonl	Raw design outputs
scene2_sft.jsonl	Cleaned SFT dataset (Scene-2)
combined_sft.jsonl	Final merged dataset
âœ… Assignment Alignment

This project satisfies:

âœ” Two real scenarios (Q&A + architecture design)
âœ” Automated training-data pipeline
âœ” Reusable, extensible design
âœ” Works on open-source or private repositories
âœ” Produces reasoning + explanation traces
âœ” Includes dry-run validation capability

ğŸ‘€ Notes

Large repositories may take longer to scan

Avoid scanning folders like node_modules, build, .git

Always test with DRY_RUN first
